# @package _global_
optimizer:
  _target_: torch.optim.AdamW
  lr: ${training.lr}
