# @package _global_
scheduler:
  _target_: torch.optim.lr_scheduler.CyclicLR
  base_lr: ${training.lr}
  max_lr: 0.1